{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bigboyz - Dubai Real Estate Price Prediction\n",
    "\n",
    "**CSCI316: Big Data Mining Techniques and Implementation**  \n",
    "University of Wollongong in Dubai\n",
    "\n",
    "---\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook implements a large-scale machine learning pipeline to predict real estate transaction prices in Dubai using the Dubai Land Department's transactions dataset (~1.5 million records).\n",
    "\n",
    "**Business Question:** *Can we accurately predict real estate transaction prices in Dubai based on property characteristics, location, and temporal factors?*\n",
    "\n",
    "### Key Requirements\n",
    "- **10-Fold Cross-Validation**: Implemented from scratch (no CrossValidator/TrainValidationSplit)\n",
    "- **Bagging Ensemble**: Implemented from scratch (no RandomForestRegressor)\n",
    "- **Apache Spark**: All data processing uses PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Table of Contents\n",
    "\n",
    "1. [Environment Setup](#1-environment-setup)\n",
    "2. [Data Loading & Exploration](#2-data-loading--exploration)\n",
    "3. [Data Cleaning](#3-data-cleaning)\n",
    "4. [Feature Engineering](#4-feature-engineering)\n",
    "5. [Train/Test Split](#5-traintest-split)\n",
    "6. [Model Training with Manual Cross-Validation](#6-model-training-with-manual-cross-validation)\n",
    "7. [Bagging Ensemble (From Scratch)](#7-bagging-ensemble-from-scratch)\n",
    "8. [Final Evaluation on Holdout Test Set](#8-final-evaluation-on-holdout-test-set)\n",
    "9. [Results & Visualizations](#9-results--visualizations)\n",
    "10. [Conclusion](#10-conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Spark MLlib\n",
    "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Our custom modules\n",
    "from data_ingestion import create_spark_session, load_transactions, explore_data\n",
    "from data_cleaning import clean_data, analyze_missing_values\n",
    "from feature_engineering import engineer_features, train_test_split\n",
    "from cross_validation import manual_cross_validate, create_folds, compare_models_cv\n",
    "from bagging_ensemble import BaggingRegressor, bootstrap_sample\n",
    "from evaluation import (\n",
    "    calculate_metrics, evaluate_model, create_comparison_table,\n",
    "    plot_model_comparison, plot_predictions_vs_actual, \n",
    "    plot_residuals, plot_feature_importance, plot_price_distribution\n",
    ")\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark Session\n",
    "spark = create_spark_session(\"bigboyz-dubai-real-estate\")\n",
    "\n",
    "# Verify Spark is running\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark App Name: {spark.sparkContext.appName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Loading & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the transactions data\nDATA_PATH = '../data/Transactions.csv'\n\n# Load with schema inference (recommended for initial exploration)\ndf_raw = load_transactions(spark, DATA_PATH, use_schema=False)\n\nprint(f\"\\nDataset loaded successfully!\")\nprint(f\"Total records: {df_raw.count():,}\")\nprint(f\"Number of columns: {len(df_raw.columns)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display schema\n",
    "print(\"Dataset Schema:\")\n",
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample data\n",
    "print(\"Sample Data (5 rows):\")\n",
    "df_raw.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"Summary Statistics:\")\n",
    "df_raw.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze missing values\n",
    "print(\"Missing Values Analysis:\")\n",
    "missing_data = analyze_missing_values(df_raw)\n",
    "for item in missing_data:\n",
    "    if item['missing_count'] > 0:\n",
    "        print(f\"  {item['column']}: {item['missing_count']:,} ({item['missing_pct']:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define column names based on actual dataset\nPRICE_COL = 'actual_worth'  # Target variable (transaction price in AED)\n\n# Price distribution statistics\ndf_raw.select(PRICE_COL).describe().show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Column names for cleaning\nPRICE_COL = 'actual_worth'  # Target variable\nTRANSACTION_TYPE_COL = 'trans_group_en'  # Transaction type column (Sales, Gifts, Mortgages, etc.)\n\n# Run the cleaning pipeline\ndf_clean = clean_data(\n    df_raw, \n    price_col=PRICE_COL, \n    transaction_type_col=TRANSACTION_TYPE_COL\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify cleaning results\n",
    "print(f\"\\nRows after cleaning: {df_clean.count():,}\")\n",
    "print(f\"Columns after cleaning: {len(df_clean.columns)}\")\n",
    "print(f\"\\nRemaining columns: {df_clean.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot price distribution after cleaning\nplot_price_distribution(df_clean, price_col=PRICE_COL, save_path='../outputs/figures/price_distribution.png')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Feature columns based on actual dataset\n\n# Categorical columns to encode (using English columns with low missing rates)\nCATEGORICAL_COLS = [\n    'property_type_en',      # Villa, Land, Building, Unit\n    'property_usage_en',     # Residential, Commercial, etc.\n    'area_name_en',          # Dubai areas (Mankhool, Al Karama, etc.)\n    'nearest_metro_en',      # Metro station proximity\n    'nearest_mall_en',       # Mall proximity\n]\n\n# Numeric columns to include as features\nNUMERIC_COLS = [\n    'procedure_area',        # Property size in sqm\n    'has_parking',           # 0 or 1\n]\n\n# Date column for temporal features\nDATE_COL = 'instance_date'   # Format: DD-MM-YYYY\n\n# Target column\nLABEL_COL = PRICE_COL"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature engineering\n",
    "df_features, feature_names = engineer_features(\n",
    "    df_clean,\n",
    "    date_col=DATE_COL,\n",
    "    categorical_cols=CATEGORICAL_COLS,\n",
    "    numeric_cols=NUMERIC_COLS,\n",
    "    label_col=LABEL_COL\n",
    ")\n",
    "\n",
    "print(f\"\\nFeature-engineered dataset:\")\n",
    "print(f\"  Rows: {df_features.count():,}\")\n",
    "print(f\"  Features: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the feature columns\n",
    "print(\"Feature columns:\")\n",
    "for i, name in enumerate(feature_names):\n",
    "    print(f\"  {i}: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 80/20 train/test split\n",
    "# The test set will be our HOLDOUT set - never touched during training/CV\n",
    "\n",
    "df_train, df_holdout = train_test_split(df_features, train_ratio=0.8, seed=42)\n",
    "\n",
    "# Cache training data for efficiency\n",
    "df_train.cache()\n",
    "print(f\"\\nTraining data cached: {df_train.count():,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Model Training with Manual Cross-Validation\n",
    "\n",
    "**IMPORTANT:** This section uses our custom `manual_cross_validate()` function, which implements 10-fold cross-validation from scratch WITHOUT using Spark MLlib's `CrossValidator` or `TrainValidationSplit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model builder functions\n",
    "# These functions return fresh (untrained) model instances\n",
    "\n",
    "def linear_regression_builder():\n",
    "    return LinearRegression(\n",
    "        featuresCol='features',\n",
    "        labelCol='label',\n",
    "        predictionCol='prediction',\n",
    "        maxIter=100,\n",
    "        regParam=0.1\n",
    "    )\n",
    "\n",
    "def decision_tree_builder():\n",
    "    return DecisionTreeRegressor(\n",
    "        featuresCol='features',\n",
    "        labelCol='label',\n",
    "        predictionCol='prediction',\n",
    "        maxDepth=10\n",
    "    )\n",
    "\n",
    "def random_forest_builder():\n",
    "    return RandomForestRegressor(\n",
    "        featuresCol='features',\n",
    "        labelCol='label',\n",
    "        predictionCol='prediction',\n",
    "        numTrees=20,\n",
    "        maxDepth=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all models to compare\n",
    "model_configs = {\n",
    "    'Linear Regression': linear_regression_builder,\n",
    "    'Decision Tree': decision_tree_builder,\n",
    "    'Random Forest (baseline)': random_forest_builder,\n",
    "}\n",
    "\n",
    "# Run manual 10-fold CV for all baseline models\n",
    "cv_results = compare_models_cv(df_train, model_configs, k=10, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Bagging Ensemble (From Scratch)\n",
    "\n",
    "**IMPORTANT:** This section uses our custom `BaggingRegressor` class, which implements bagging from scratch WITHOUT using `RandomForestRegressor`. We use `DecisionTreeRegressor` as base learners and manually implement:\n",
    "- Bootstrap sampling\n",
    "- Training multiple trees\n",
    "- Averaging predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bagging model builder\n",
    "def bagging_builder():\n",
    "    return BaggingRegressor(\n",
    "        n_estimators=10,\n",
    "        max_depth=10,\n",
    "        seed=42,\n",
    "        features_col='features',\n",
    "        label_col='label'\n",
    "    )\n",
    "\n",
    "# Run manual 10-fold CV for Bagging\n",
    "print(\"Evaluating Custom Bagging Ensemble with 10-Fold CV...\")\n",
    "bagging_cv_results = manual_cross_validate(df_train, bagging_builder, k=10, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Bagging results to comparison\n",
    "cv_results['Bagging (ours)'] = bagging_cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comparison table\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CROSS-VALIDATION RESULTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "comparison_df = create_comparison_table(cv_results)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CV results comparison\n",
    "plot_model_comparison(cv_results, save_path='../outputs/figures/cv_results.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Final Evaluation on Holdout Test Set\n",
    "\n",
    "Now we train final models on the full training set (80%) and evaluate on the holdout test set (20%) that was never seen during cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final models on full training set\n",
    "print(\"Training final models on full training set...\")\n",
    "\n",
    "# Linear Regression\n",
    "lr_model = linear_regression_builder().fit(df_train)\n",
    "print(\"  Linear Regression trained.\")\n",
    "\n",
    "# Decision Tree\n",
    "dt_model = decision_tree_builder().fit(df_train)\n",
    "print(\"  Decision Tree trained.\")\n",
    "\n",
    "# Random Forest (baseline)\n",
    "rf_model = random_forest_builder().fit(df_train)\n",
    "print(\"  Random Forest trained.\")\n",
    "\n",
    "# Bagging (our custom implementation)\n",
    "bagging_model = bagging_builder()\n",
    "bagging_model.fit(df_train, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on holdout test set\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL EVALUATION ON HOLDOUT TEST SET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_results = {}\n",
    "\n",
    "# Linear Regression\n",
    "_, lr_metrics = evaluate_model(lr_model, df_holdout, 'Linear Regression')\n",
    "test_results['Linear Regression'] = lr_metrics\n",
    "\n",
    "# Decision Tree\n",
    "_, dt_metrics = evaluate_model(dt_model, df_holdout, 'Decision Tree')\n",
    "test_results['Decision Tree'] = dt_metrics\n",
    "\n",
    "# Random Forest\n",
    "_, rf_metrics = evaluate_model(rf_model, df_holdout, 'Random Forest (baseline)')\n",
    "test_results['Random Forest (baseline)'] = rf_metrics\n",
    "\n",
    "# Bagging (custom)\n",
    "bagging_predictions = bagging_model.predict(df_holdout)\n",
    "bagging_test_metrics = calculate_metrics(bagging_predictions)\n",
    "test_results['Bagging (ours)'] = bagging_test_metrics\n",
    "print(f\"\\n{'=' * 50}\")\n",
    "print(f\"EVALUATION RESULTS: Bagging (ours)\")\n",
    "print(f\"{'=' * 50}\")\n",
    "print(f\"  RMSE: {bagging_test_metrics['rmse']:,.2f}\")\n",
    "print(f\"  MAE:  {bagging_test_metrics['mae']:,.2f}\")\n",
    "print(f\"  R²:   {bagging_test_metrics['r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final comparison table\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL TEST SET RESULTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "test_comparison_df = create_comparison_table(test_results)\n",
    "print(test_comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot test results comparison\n",
    "plot_model_comparison(test_results, save_path='../outputs/figures/model_comparison.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Results & Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actual for best model (Bagging)\n",
    "print(\"Predictions vs Actual (Bagging Ensemble):\")\n",
    "plot_predictions_vs_actual(bagging_predictions, sample_size=10000, \n",
    "                          save_path='../outputs/figures/predictions_vs_actual.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residuals for Bagging model\n",
    "print(\"Residual Analysis (Bagging Ensemble):\")\n",
    "plot_residuals(bagging_predictions, sample_size=10000,\n",
    "              save_path='../outputs/figures/residuals.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from Bagging ensemble\n",
    "print(\"Feature Importance (Bagging Ensemble):\")\n",
    "importance = bagging_model.get_feature_importance(feature_names)\n",
    "plot_feature_importance(importance, top_n=15, \n",
    "                       save_path='../outputs/figures/feature_importance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Conclusion\n",
    "\n",
    "### Summary of Results\n",
    "\n",
    "| Model | CV RMSE | CV R² | Test RMSE | Test R² |\n",
    "|-------|---------|-------|-----------|--------|\n",
    "| Linear Regression | ... | ... | ... | ... |\n",
    "| Decision Tree | ... | ... | ... | ... |\n",
    "| Random Forest (baseline) | ... | ... | ... | ... |\n",
    "| **Bagging (ours)** | ... | ... | ... | ... |\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **TODO**: Fill in observations about model performance\n",
    "2. **TODO**: Discuss feature importance insights\n",
    "3. **TODO**: Compare custom Bagging vs Random Forest baseline\n",
    "\n",
    "### Lessons Learned\n",
    "\n",
    "1. **Manual Cross-Validation**: Implementing 10-fold CV from scratch provided deeper understanding of...\n",
    "2. **Bagging Ensemble**: Building bagging from scratch showed...\n",
    "3. **Big Data Considerations**: Working with 1.5M records using Spark..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "df_train.unpersist()\n",
    "spark.stop()\n",
    "print(\"Spark session stopped. Notebook complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}